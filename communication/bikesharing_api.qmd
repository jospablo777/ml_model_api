---
title: "Bike Rentals Prediction API"
author:
  name: "Jose P. Barrantes"
  url: "https://www.linkedin.com/in/jose-barrantes/"
description: "GitHub Repo: [Bike Rentals Prediction API](https://github.com/jospablo777/ml_model_api)"
format: 
  html:
    toc: true
    toc-location: right
    toc-title: "On this page"
    aside: true
editor: source
---

```{r}
#| label: setup
#| include: false
#| output: false

# Tell quarto where is our python venv
Sys.setenv(RETICULATE_PYTHON = "../venv/bin/python")
```


![](img/pondering_orb_meme.png)

## An accesible inference system

In a fast-paced world, one of the problems we face as data professionals is delivering our data products promptly. Also, distributing them in a manner that doesn't affect our productivity, for example, manually running a Jupyter Notebook or an R script every time someone needs insights, is awfully unproductive.

An idea that is not new, is to deliver the "answers" in an automated, from which the clients can self-serve each time they are in need. After we have a data product, for instance, a predictive model (*machine learning* if you are into buzzwords), we can make available its capabilities to our stakeholders via a micro service.

## An ML model as a microservice

Imagine your (predictive) model being accessed anytime without disturbing your peace or consuming your time; after all, your time is expensive, and you have many tasks. Here is where the idea of microservices comes into action. A microservice is an isolated piece of software that is in charge of a single task (service) and communicates through a well-defined API. You finish your model, pack it in a microservice, and make it available through an API.

This way, the model will always be available, and its consumption won't block other tasks. Also, more products could be potentially developed with the API.

## But what is an API?

An API is an intermediary that enables you to interact with a service without requiring knowledge of how this service works. You just need to know precisely what you want, tell it to the API, and the API will serve you what you requested.

Let's say that after work, you get hungry, so you park your Capital Bikeshare™ bike out of your favorite Mexican restaurant[^1]. You enter the restaurant and sit at a table. Instead of going directly to the kitchen to cook your own meal or asking the chef what you want, you need to place your order through the waiter or waitress. You kindly request your enchiladas, and after a while, they bring your meal to you.

![Allegory of a restaurant. The waiter/waitress is the API; the kitchen is the service (they are dedicated to cooking); you are the client](img/restaurant_api.png)

Notice that you, as the client, are not required to know how to cook enchiladas. Also, you do not interact directly with the people who prepare the food. You get what you want through a waiter (the API).

In summary, an API is the piece of the system you interact with to get a result generated by a service. Producing this result might be complex, but you don't care about that since you only have to deal with the API.

[^1]: This must be how George R. R. Martin feels when foreshadowing a major (traumatic) event in his novels.

## Software engineering for data scientists

Another problem common to data scientists is that research, predictive modeling, and analytics are dirty processes. This is partly due to a lack of training in **software engineering**, which involves **design**, testing, and software maintenance.

We usually have many problems to solve, questions to answer, and limited time at work. This leaves little to no time to apply the best practices to develop our analysis and maintain our code, leaving a trail of technical debt with each delivery. Yet, since most of us write code, we are, in fact, software developers because we develop software. This should be a hard-to-miss hint that software engineering practices must be core in our profession, or at least not ignored.

Here, we will implement some practices core of software engineering like:

- **CI/CD:** Continuous Integration (CI) and Continuous Deployment (CD) with GitHub Actions.
- **Automated tests:** unit, integration, and inference tests with `pytest`.
- **Containerization:** package the application with Docker for consistency across environments (i.e., it should work on every machine with a well-setup Docker).
Dependency Management: For reproducibility, dependencies were specified in a `requirements.txt` file, and development was conducted in an isolated environment (`venv`).
- **Microservices architecture:** The project is a self-contained microservice focused on a single responsibility: predicting bike rentals. It is independently deployable and communicates through HTTP.

We intend to review these concepts in a practical, applied way. This will give us more tools to design and produce more sustainable systems, which we can develop more efficiently through good practices without sacrificing smooth delivery to our clients.

## About the technologies

Through this practical project, we will deal with several technologies, the most important:

- **FastAPI:** a framework to develop APIs with Python. It is easy to learn, and it makes the development of APIs incredibly fast.
- **Uvicorn:** an ASGI (Asynchronous Server Getaway Interface) Server. It handles incoming HTTP requests and sends responses. This lightweight server is appropriate for our microservice.
- **Docker:** a platform to pack our application into isolated "containers" and make them available to be run in several systems. Perfect to build microservices.
- **GitHub Actions:** a CI/CD platform that allows us to automate your development pipelines. We will use them here to automate the software tests and build and push our docker images.
- **pytest:** a Python framework to simplify our software testing.

These tools will make the development and publishing of our microservice a breeze.

## Let's get started

We will start with the mandatory xkcd comic that this kind of articles include so we can continue with the tutorial.

![Exploits of a Mom, by xkcd  Comics. Available at https://xkcd.com/3027/](img/exploits_of_a_mom.png)

I know that this has nothing to do with software engineering or APIs, but little Bobby Tables always cracks me laugh. Now we can continue with the build of our API and some software engineering practices.

## Problem we want to solve

Now, let's focus on the problem we want to solve; this is probably the most crucial step since it is how we **add value** to the business. We are hired to solve analytical problems with evidence (data) as our raw material to produce those solutions.

You probably won't be surprised to learn this project started as a Jupyter Notebook, which I encourage you to [check out here](https://github.com/jospablo777/ml_model_api/blob/main/notebooks/eda_and_toy_model.ipynb). We are using an open data set from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset).

Let's assume we are working for [Capital Bikeshare](https://capitalbikeshare.com/), a company dedicated to providing a self-service bike rental service managed through a mobile app. For the company, it is crucial to predict the peaks of service usage so they can restock or make more available their products when they're more needed. These products can be cloud computing resources during peak hours, bicycle units, human staff, etc. Also it can b convenient to predict the dates and hours of least use for maintenance tasks, like server updates or bike maintenance.

![Capital Bikeshare station outside of the Eastern Market Metro station. Author: Ben Schumin (2010). Licensed under CC BY-SA 3.0](img/Capital_Bikeshare_station_outside_Eastern_Market_Metro.jpg)

Working as a data scientist for a big tech transport company, you will have lots of data and computational resources. On this occasion, you were put in charge of finding a way to predict the days and hours they will have to replenish their stock and increase the server's capacity, investing in more resources only when necessary (saving money). Also, knowing when to schedule the maintenance labor will be valuable as well.

After finishing the data exploration, you know that working days and the entry and exit from work hours are the busiest time frames. Also, climatic conditions can affect the willingness to rent a bike. You are cautious, so you first took the time to understand the phenomenon. Then, you built the model prototype and corroborated that it is possible to forecast the number of rented bikes at an hour-of-the-day granularity level.

Success! You have a model that can help determine the best time to provide more resources or the times for maintenance. Does this mean that you have already finished with the task and can go on to live the good life?[^2] Well, you already know that Jupyter is not a tool that produces clean results and that if you leave it there, your model will have to be run manually each time, which can introduce bugs and errors. This also means that you will have a high and constant influx of people knocking at your door[^3] asking for predictions and insights.

Jupyter is ideal for experimenting and testing but not for delivering results. Hence, you have decided to deploy this first iteration of the model (the prototype) as a microservice that can be consumed via an API. Here, you can assess how the model behaves in the wild and prepare for future iterations. You will start adding value with predictions and valuable information that will help improve your model. 

The engineering team, for example, will quickly adopt the app to know when to conduct maintenance jobs.

[^2]: At the time of writing, I am aware that "no" is the answer since we're still just at the introduction of this tutorial. 
[^3]: It's even worse today, in our modern remote era. The dreaded Microsoft Teams ringtone will constantly haunt you.

## Microservice development

We were assigned a task, so we started by investigating bike rental data and then continued by building a predictive model using the [CatBoost](https://catboost.ai/) algorithm. This model will be the inference engine of our microservice. You can see this part of the project [here](https://github.com/jospablo777/ml_model_api/blob/main/notebooks/eda_and_toy_model.ipynb) in the notebook[^4]. We saved the model in our project's `predictive_models/` folder.

:::{.callout-note}
During this tutorial, we will show you some code snippets and indicate the file where the code is located[^5]. You should then be able to follow the project's code, which is available on the [GitHub repo](https://github.com/jospablo777/ml_model_api).
:::

After some research and experimentation, we developed a CatBoost model capable of predicting the number of rented bikes at a specific time, given some environmental and temporal conditions. The code that generated the model is next.

Filename: [`notebooks/eda_and_toy_model.ipynb`](https://github.com/jospablo777/ml_model_api/blob/main/notebooks/eda_and_toy_model.ipynb)
```{python}
#| label: predictive_model
#| include: true
#| eval: false

from catboost import CatBoostRegressor

# --snip--

# Instantiate our model
catboost_model = CatBoostRegressor(
    iterations=1000,       # Boosting iterations
    learning_rate=0.1,
    depth=6,               # Tree depth
    loss_function='RMSE',  # Loss
    verbose=100            # Let it print the learning state every 100 iterations
)

# Train
catboost_model.fit(X_train, y_train, cat_features=categorical_features)

# --snip--

# Save trained model
catboost_model.save_model("../predictive_models/catboost_model_19Dec2024.cbm")
```

In [this analysis](https://github.com/jospablo777/ml_model_api/blob/main/notebooks/eda_and_toy_model.ipynb), we generated some insights[^6] and the core of our microservice, a predictive model stored in `predictive_models/catboost_model_19Dec2024.cbm`. Now that we have our predictor, let's see how we can make the predictions accessible to others in the company.

[^4]: You probably already noticed that we keep bringing up the link to this notebook. That's because we spent so much time on it, and we was hoping you could look at our precious.
[^5]: I learned this tip for technical writing from "The Rust Programming Language" book by Steve Klabnik, Carol Nichols, and the Rust Community. The book is a masterpiece and a highly recommended reading; you can find it [here](https://doc.rust-lang.org/book/title-page.html).
[^6]: Whoops, here is the notebook again. When presenting results to my stakeholders as a habit, I always prepare two decks, one with very high-level results and a second highly technical one with all the math of the statistical/ML models and intricacies of the system design, just in case somebody has a question on the [interesting]{style="text-decoration: line-through;"}  technical part. This is expecting a deep, nerdy discussion about models, gaps, opportunities, and science; to date, the counts for the times I've used this second deck remain at 0. The good news is that it might be a proxy of our stakeholders' trust in us.


## API development

The API will move data around (receive and deliver) as its primary job, so it is key to establish a clear set of rules for the data our clients send. Moving back to our restaurant example, you probably could put your enchiladas order in English or Spanish, but it might be hard for the waiter if you place the order in Japanese[^7]. Hence, to get your enchiladas, you must request them in a way that the staff can understand, and [Pydantic](https://docs.pydantic.dev/latest/) is here to enforce the rules that make such understanding happen.

![An ambiguous (and invalid) request request to the API, so no enchiladas :(](img/restaurant_api_jap.png)

### The Pydantic model

We will start this section by defining our Pydantic model. Pydantic is a library for data validation that uses Python-type annotations. Let's establish a Pydantic model for our API requests and learn more about the library in the process.

Filename: [`app/models/bike_sharing.py`](https://github.com/jospablo777/ml_model_api/blob/main/app/models/bike_sharing.py)
```{python}
#| label: pydantic_model
#| include: true
#| eval: false

from pydantic import BaseModel, Field, field_validator

class BikeSharingRequest(BaseModel):                                           # <1>                     
    season: str = Field(..., description="Season (Winter/Spring/Summer/Fall)") # <2>
    mnth: str = Field(..., description="Month name (January, etc.)")
    hr: int = Field(..., ge=0, le=23, description="Hour of the day")           # <3>
    holiday: str = Field(..., description="Yes/No if holiday")
    weekday: str = Field(..., description="Name of the weekday (Monday/Tuesday/Wednesday/Thursday/Friday/Saturday/Sunday)")
    workingday: str = Field(..., description="Yes/No if working day")          # <4>
    weathersit: int = Field(..., description="Weather code")
    temp: float = Field(..., ge=0, le=1, description="Normalized temperature")
    atemp: float = Field(..., ge=0, le=1, description="Normalized feeling temperature")
    hum: float = Field(..., ge=0, le=1, description="Normalized humidity")
    windspeed: float = Field(..., ge=0, le=1, description="Normalized wind speed")

    @field_validator("season")   # <5>
    def validate_season(cls, v): # <6>
        allowed = {"Winter", "Spring", "Summer", "Fall"} 
        if v not in allowed:                             # <7>
            raise ValueError(f"season must be one of {allowed}") # <8>
        return v

# -- snip -- (validators continue)
```

1. The first thing to notice is that the Pydantic model is defined as a Python class that inherits from `BaseModel`; this is the Pydantic base class used to create models with built-in data validation.
2. Each class attribute is defined with a data type; `season`, for example, is defined as `str`. We also use the function `Field()` to specify constraints and metadata; the ellipsis literal `...` here means two things: first, a value must be provided, and second, there is no default value for this attribute. Then, some metadata is specified in the `description` parameter.
3. For `hr` attribute, we have constraints used for numeric validation. So, `ge=0` means that the hour has to be greater or equal to 0, while `le=23` indicates that the hour should be less or equal to 23. So if the user send a request with `hr=-1` the system will complain and throw a warning.
4. With what we know so far, we could read this line as "`workingday` is a required `str` field with a description."
5. We have the `field_validator` decorator[^8] to build custom validation logic for a specific field. Besides the already fantastic default capabilities of Pydantic, we can define our custom validations. In this case, we want to be sure that the client will provide only valid values for the `season` field. CatBoost uses categories as predictors, and it is case sensitive (i.e., `"Summer" != "summer`). With this extra step, FastAPI will send a custom message to the client, informing that the API expects `"Summer"` not `"summer"`.
6. We define the function `validate_season`, which takes two parameters: `cls`, a keyword indicating a reference to the class (`BikeSharingRequest`), and `v`, representing the value of the field we're validating.
7. Check if the value we validate is within the `allowed` set.
8. Raise an error if the value is not allowed; it also tells the client which values are expected.

To summarize the structure of our Pydantic model:

Filename: [`app/models/bike_sharing.py`](https://github.com/jospablo777/ml_model_api/blob/main/app/models/bike_sharing.py)
```{python}
#| label: pydantic_model_general_description
#| include: true
#| eval: false

from pydantic import BaseModel, Field, field_validator                         # <1>  

class BikeSharingRequest(BaseModel):                                           # <2>
    season: str = Field(..., description="Season (Winter/Spring/Summer/Fall)") # <3>
    mnth: str = Field(..., description="Month name (January, etc.)")           # <3>
    hr: int = Field(..., ge=0, le=23, description="Hour of the day")           # <3>
    holiday: str = Field(..., description="Yes/No if holiday")                 # <3>
    weekday: str = Field(..., description="Name of the weekday (Monday/Tuesday/Wednesday/Thursday/Friday/Saturday/Sunday)")                   # <3>
    workingday: str = Field(..., description="Yes/No if working day")          # <3>
    weathersit: int = Field(..., description="Weather code")                   # <3>
    temp: float = Field(..., ge=0, le=1, description="Normalized temperature") # <3>
    atemp: float = Field(..., ge=0, le=1, description="Normalized feeling temperature") # <3>
    hum: float = Field(..., ge=0, le=1, description="Normalized humidity")     # <3>
    windspeed: float = Field(..., ge=0, le=1, description="Normalized wind speed") # <3>

    @field_validator("season")                                   # <4>
    def validate_season(cls, v):                                 # <4>
        allowed = {"Winter", "Spring", "Summer", "Fall"}         # <4>
        if v not in allowed:                                     # <4>
            raise ValueError(f"season must be one of {allowed}") # <4>
        return v                                                 # <4>

# -- snip -- (validators continue)
```
1. Required Pydantic imports.
2. The model is defined as a Python class.
3. Within the class, we define the model's fields as attributes of the class
4. We can include custom validators within the model.

Great! Now, we have a Pydantic model and a better understanding of it. Next, we will work on our API.

[^7]: エンチラーダをいただけますか？
[^8]: A decorator is a function whose intention is to modify the behavior of other functions; it takes the original function and converts it into a different function. Luciano Ramalho does a great job explaining decorators in chapter 9 of his book ["Fluent Python"](https://www.oreilly.com/library/view/fluent-python-2nd/9781492056348/).

### FastAPI development

------- All the text below needs LOTS of editing after I had some sleep ------------------

With the data model in place (the class `BikeSharingRequest`), we can continue developing our microservice served via an API.

Filename: [`app/main.py`](https://github.com/jospablo777/ml_model_api/blob/main/app/main.py)
```{python}
#| label: fastapi_app_0
#| include: true
#| eval: false

# -- snip --

# Instantiate a FastAPI class, the core of the application
app = FastAPI()                                                         # <1>

# Read predictive model
ml_model = load_model('predictive_models/catboost_model_19Dec2024.cbm') # <2> 

# Defines a GET endpoin at the root path "/"
@app.get("/")                                                           # <3>
async def index() -> dict:
    return {"message": "Bike rentals ML predictor (regressor)"} # Returns a simple JSON response with a message

# --snip--
```
1. Instantiates the main FastAPI application. Every endpoint and configuration will attach to this `app` object.
2. Read the model.
3. A GET endpoint to the root URL

Filename: [`app/main.py`](https://github.com/jospablo777/ml_model_api/blob/main/app/main.py)
```{python}
#| label: fastapi_app_1
#| include: true
#| eval: false

# -- snip --

# Defines a POST endpoint for predictions
@app.post('/predict',                                                                           # <1>
          summary = "Predict bike rentals",                                                     # <1>
          description = "Takes input data and returns amount rental predictions (regression).") # <1>
async def predict_rentals(requests: List[BikeSharingRequest]) -> dict:           # <2>
    """
    Predict bike rentals based on input features.

    Args:
        requests (List[BikeSharingRequests]): A list of input data objects.

    Returns:
        dict: A dictionary containing the predictions as a list.
    """

    # Handles the case when an empty list is posted
    if not requests:                         # <3>
        raise HTTPException(                 # <3>
            status_code = 422,               # <3>
            detail = "The request is empty"  # <3>
        )                                    # <3>
    
    # Convert each pydantic model to a dict and load into a Polars DataFrame
    df = req_to_polars(requests)             # <4>

    # Instances to predict
    features = df.to_numpy()                 # <5>

    # Make predictions
    predictions = ml_model.predict(features) # <6>

    return {
        'predictions': predictions.tolist()  # <7>
    }
```
1. Defines a POST endpoint at `/predict`. Some metada is included with the parameters `summary` and `description.
2. Data validation is ensured since function receives a list of `BikeSharingRequest`.
3. If request is an empty list `[]`, FastAPI will return the "The request is empty" messagee to the client
4. Converts the list of `BikeSharingRequest` into a polars data frame.
5. To date of writing, CatBoost do not accept polars data frames as input, so we convert to a list of numpy arrays.
6. Generates predictions.
7. Returns a JSON.

### Pydantic model

If you navigate to our FastAPI application documentation (http://127.0.0.1:8000/docs) you will be able to see the Pydantic model used for data validation, even the `description` of the fields especified as attributes in the class `BikeSharingRequest`. Here you will be able to read the default constraints (data type), and the ones specified with the `Field()` function. The numeric restrictions we indicated appear between the square brackets(e.g. `[0, 23]` for the `hr` field) Custom constraints are no automatically displayed, yet we indicated them as metadata in in the `description` parameter.

FastAPI uses Pydantic behind the scenes to convert each JSON object in the incoming list to a BikeSharingRequest instance.
 
![Pydantic model](img/schema_pydantic_model.png)

## Automated tests

## Containarization

## Inference

## Microservice up and running

## What we learned?

## Whats left?
Model monitoring, automatic training incorporated to the CI/CD, model versioning, high level applications that consume the APIs

## Recommended readings

### Microservice APIs by Jose Haro Peralta

If you're new to APIs and microservices, [Microservice APIs](https://www.manning.com/books/microservice-apis) by Jose Haro Peralta provides an excellent starting point. The author presents fundamental software architecture and engineering principles in an accessible, hands-on way. You'll find practical code examples to experiment with, paired with clear theoretical explanations that help you understand both the "how" and the "why" behind the code.

### Docker Deep Dive by Nigel Poulton

For anyone looking to learn Docker, Nigel Poulton's [Docker Deep Dive](https://www.amazon.com/Docker-Deep-Dive-Zero-single/dp/191658537X) offers a concise yet insightful introduction. Despite its brevity, the book covers many concepts and provides straightforward exercises. By following along with the examples, you'll quickly gain the confidence to use Docker effectively in real-world scenarios.

### Software Engineering for Data Scientists by Andrew Treadway

Currently available as a MEAP (Manning Early Access Program), [Software Engineering for Data Scientists](https://www.manning.com/books/software-engineering-for-data-scientists) by Andrew Treadway is a remarkable resource for data professionals eager to strengthen their software engineering skills. The book’s in-progress chapters seamlessly blend theory and practice, offering well-researched explanations and practical examples. This is a must-read if you want to elevate your data projects with solid engineering principles.



## About the pondering guy in the header

It's the wizard from "Pondering My Orb." The art is the work by Angus Mcbride and was featured in a Lord of the Rings game book :) you can check [Know Your Meme](https://knowyourmeme.com/memes/pondering-my-orb) for more info/memes.

