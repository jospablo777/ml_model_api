[
  {
    "objectID": "bikesharing_api.html#an-accesible-inference-system",
    "href": "bikesharing_api.html#an-accesible-inference-system",
    "title": "Bike Rentals Prediction API",
    "section": "An accesible inference system",
    "text": "An accesible inference system\nIn a fast-paced world, one of the problems we face as data professionals is delivering our data products promptly. Also, distributing them in a manner that doesn‚Äôt affect our productivity, for example, manually running a Jupyter Notebook or an R script every time someone needs insights, is awfully unproductive.\nAn idea that is not new, is to deliver the ‚Äúanswers‚Äù in an automated, from which the clients can self-serve each time they are in need. After we have a data product, for instance, a predictive model (machine learning if you are into buzzwords), we can make available its capabilities to our stakeholders via a micro service."
  },
  {
    "objectID": "bikesharing_api.html#an-ml-model-as-a-microservice",
    "href": "bikesharing_api.html#an-ml-model-as-a-microservice",
    "title": "Bike Rentals Prediction API",
    "section": "An ML model as a microservice",
    "text": "An ML model as a microservice\nImagine your (predictive) model being accessed anytime without disturbing your peace or consuming your time; after all, your time is expensive, and you have many tasks. Here is where the idea of microservices comes into action. A microservice is an isolated piece of software that is in charge of a single task (service) and communicates through a well-defined API. You finish your model, pack it in a microservice, and make it available through an API.\nThis way, the model will always be available, and its consumption won‚Äôt block other tasks. Also, more products could be potentially developed with the API."
  },
  {
    "objectID": "bikesharing_api.html#but-what-is-an-api",
    "href": "bikesharing_api.html#but-what-is-an-api",
    "title": "Bike Rentals Prediction API",
    "section": "But what is an API?",
    "text": "But what is an API?\nAn API is an intermediary that enables you to interact with a service without requiring knowledge of how this service works. You just need to know precisely what you want, tell it to the API, and the API will serve you what you requested.\nLet‚Äôs say that after work, you get hungry, so you park your Capital Bikeshare‚Ñ¢ bike out of your favorite Mexican restaurant1. You enter the restaurant and sit at a table. Instead of going directly to the kitchen to cook your own meal or asking the chef what you want, you need to place your order through the waiter or waitress. You kindly request your enchiladas, and after a while, they bring your meal to you.\n\n\n\nAllegory of a restaurant. The waiter/waitress is the API; the kitchen is the service (they are dedicated to cooking); you are the client\n\n\nNotice that you, as the client, are not required to know how to cook enchiladas. Also, you do not interact directly with the people who prepare the food. You get what you want through a waiter (the API).\nIn summary, an API is the piece of the system you interact with to get a result generated by a service. Producing this result might be complex, but you don‚Äôt care about that since you only have to deal with the API."
  },
  {
    "objectID": "bikesharing_api.html#software-engineering-for-data-scientists",
    "href": "bikesharing_api.html#software-engineering-for-data-scientists",
    "title": "Bike Rentals Prediction API",
    "section": "Software engineering for data scientists",
    "text": "Software engineering for data scientists\nAnother problem common to data scientists is that research, predictive modeling, and analytics are dirty processes. This is partly due to a lack of training in software engineering, which involves design, testing, and software maintenance.\nWe usually have many problems to solve, questions to answer, and limited time at work. This leaves little to no time to apply the best practices to develop our analysis and maintain our code, leaving a trail of technical debt with each delivery. Yet, since most of us write code, we are, in fact, software developers because we develop software. This should be a hard-to-miss hint that software engineering practices must be core in our profession, or at least not ignored.\nHere, we will implement some practices core of software engineering like:\n\nCI/CD: Continuous Integration (CI) and Continuous Deployment (CD) with GitHub Actions.\nAutomated tests: unit, integration, and inference tests with pytest.\nContainerization: package the application with Docker for consistency across environments (i.e., it should work on every machine with a well-setup Docker). Dependency Management: For reproducibility, dependencies were specified in a requirements.txt file, and development was conducted in an isolated environment (venv).\nMicroservices architecture: The project is a self-contained microservice focused on a single responsibility: predicting bike rentals. It is independently deployable and communicates through HTTP.\n\nWe intend to review these concepts in a practical, applied way. This will give us more tools to design and produce more sustainable systems, which we can develop more efficiently through good practices without sacrificing smooth delivery to our clients."
  },
  {
    "objectID": "bikesharing_api.html#about-the-technologies",
    "href": "bikesharing_api.html#about-the-technologies",
    "title": "Bike Rentals Prediction API",
    "section": "About the technologies",
    "text": "About the technologies\nThrough this practical project, we will deal with several technologies, the most important:\n\nFastAPI: a framework to develop APIs with Python. It is easy to learn, and it makes the development of APIs incredibly fast.\nUvicorn: an ASGI (Asynchronous Server Getaway Interface) Server. It handles incoming HTTP requests and sends responses. This lightweight server is appropriate for our microservice.\nDocker: a platform to pack our application into isolated ‚Äúcontainers‚Äù and make them available to be run in several systems. Perfect to build microservices.\nGitHub Actions: a CI/CD platform that allows us to automate your development pipelines. We will use them here to automate the software tests and build and push our docker images.\npytest: a Python framework to simplify our software testing.\n\nThese tools will make the development and publishing of our microservice a breeze."
  },
  {
    "objectID": "bikesharing_api.html#lets-get-started",
    "href": "bikesharing_api.html#lets-get-started",
    "title": "Bike Rentals Prediction API",
    "section": "Let‚Äôs get started",
    "text": "Let‚Äôs get started\nWe will start with the mandatory xkcd comic that this kind of articles include so we can continue with the tutorial.\n\n\n\nExploits of a Mom, by xkcd Comics. Available at https://xkcd.com/3027/\n\n\nI know that this has nothing to do with software engineering or APIs, but little Bobby Tables always cracks me laugh. Now we can continue with the build of our API and some software engineering practices."
  },
  {
    "objectID": "bikesharing_api.html#problem-we-want-to-solve",
    "href": "bikesharing_api.html#problem-we-want-to-solve",
    "title": "Bike Rentals Prediction API",
    "section": "Problem we want to solve",
    "text": "Problem we want to solve\nNow, let‚Äôs focus on the problem we want to solve; this is probably the most crucial step since it is how we add value to the business. We are hired to solve analytical problems with evidence (data) as our raw material to produce those solutions.\nYou probably won‚Äôt be surprised to learn this project started as a Jupyter Notebook, which I encourage you to check out here. We are using an open data set from the UCI Machine Learning Repository.\nLet‚Äôs assume we are working for Capital Bikeshare, a company dedicated to providing a self-service bike rental service managed through a mobile app. For the company, it is crucial to predict the peaks of service usage so they can restock or make more available their products when they‚Äôre more needed. These products can be cloud computing resources during peak hours, bicycle units, human staff, etc. Also it can b convenient to predict the dates and hours of least use for maintenance tasks, like server updates or bike maintenance.\n\n\n\nCapital Bikeshare station outside of the Eastern Market Metro station. Author: Ben Schumin (2010). Licensed under CC BY-SA 3.0\n\n\nWorking as a data scientist for a big tech transport company, you will have lots of data and computational resources. On this occasion, you were put in charge of finding a way to predict the days and hours they will have to replenish their stock and increase the server‚Äôs capacity, investing in more resources only when necessary (saving money). Also, knowing when to schedule the maintenance labor will be valuable as well.\nAfter finishing the data exploration, you know that working days and the entry and exit from work hours are the busiest time frames. Also, climatic conditions can affect the willingness to rent a bike. You are cautious, so you first took the time to understand the phenomenon. Then, you built the model prototype and corroborated that it is possible to forecast the number of rented bikes at an hour-of-the-day granularity level.\nSuccess! You have a model that can help determine the best time to provide more resources or the times for maintenance. Does this mean that you have already finished with the task and can go on to live the good life?2 Well, you already know that Jupyter is not a tool that produces clean results and that if you leave it there, your model will have to be run manually each time, which can introduce bugs and errors. This also means that you will have a high and constant influx of people knocking at your door3 asking for predictions and insights.\nJupyter is ideal for experimenting and testing but not for delivering results. Hence, you have decided to deploy this first iteration of the model (the prototype) as a microservice that can be consumed via an API. Here, you can assess how the model behaves in the wild and prepare for future iterations. You will start adding value with predictions and valuable information that will help improve your model.\nThe engineering team, for example, will quickly adopt the app to know when to conduct maintenance jobs."
  },
  {
    "objectID": "bikesharing_api.html#microservice-development",
    "href": "bikesharing_api.html#microservice-development",
    "title": "Bike Rentals Prediction API",
    "section": "Microservice development",
    "text": "Microservice development\nWe were assigned a task, so we started by investigating bike rental data and then continued by building a predictive model using the CatBoost algorithm. This model will be the inference engine of our microservice. You can see this part of the project here in the notebook4. We saved the model in our project‚Äôs predictive_models/ folder.\n\n\n\n\n\n\nNote\n\n\n\nDuring this tutorial, we will show you some code snippets and indicate the file where the code is located5. You should then be able to follow the project‚Äôs code, which is available on the GitHub repo.\n\n\nAfter some research and experimentation, we developed a CatBoost model capable of predicting the number of rented bikes at a specific time, given some environmental and temporal conditions. The code that generated the model is next.\nFilename: notebooks/eda_and_toy_model.ipynb\n\nfrom catboost import CatBoostRegressor\n\n# --snip--\n\n# Instantiate our model\ncatboost_model = CatBoostRegressor(\n    iterations=1000,       # Boosting iterations\n    learning_rate=0.1,\n    depth=6,               # Tree depth\n    loss_function='RMSE',  # Loss\n    verbose=100            # Let it print the learning state every 100 iterations\n)\n\n# Train\ncatboost_model.fit(X_train, y_train, cat_features=categorical_features)\n\n# --snip--\n\n# Save trained model\ncatboost_model.save_model(\"../predictive_models/catboost_model_19Dec2024.cbm\")\n\nIn this analysis, we generated some insights6 and the core of our microservice, a predictive model stored in predictive_models/catboost_model_19Dec2024.cbm. Now that we have our predictor, let‚Äôs see how we can make the predictions accessible to others in the company."
  },
  {
    "objectID": "bikesharing_api.html#api-development",
    "href": "bikesharing_api.html#api-development",
    "title": "Bike Rentals Prediction API",
    "section": "API development",
    "text": "API development\nThe API will move data around (receive and deliver) as its primary job, so it is key to establish a clear set of rules for the data our clients send. Moving back to our restaurant example, you probably could put your enchiladas order in English or Spanish, but it might be hard for the waiter if you place the order in Japanese7. Hence, to get your enchiladas, you must request them in a way that the staff can understand, and Pydantic is here to enforce the rules that make such understanding happen.\n\n\n\nAn ambiguous (and invalid) request request to the API, so no enchiladas :(\n\n\n\nThe Pydantic model\nWe will start this section by defining our Pydantic model. Pydantic is a library for data validation that uses Python-type annotations. Let‚Äôs establish a Pydantic model for our API requests and learn more about the library in the process.\nFilename: app/models/bike_sharing.py\n\nfrom pydantic import BaseModel, Field, field_validator\n\n1class BikeSharingRequest(BaseModel):\n2    season: str = Field(..., description=\"Season (Winter/Spring/Summer/Fall)\")\n    mnth: str = Field(..., description=\"Month name (January, etc.)\")\n3    hr: int = Field(..., ge=0, le=23, description=\"Hour of the day\")\n    holiday: str = Field(..., description=\"Yes/No if holiday\")\n    weekday: str = Field(..., description=\"Name of the weekday (Monday/Tuesday/Wednesday/Thursday/Friday/Saturday/Sunday)\")\n4    workingday: str = Field(..., description=\"Yes/No if working day\")\n    weathersit: int = Field(..., description=\"Weather code\")\n    temp: float = Field(..., ge=0, le=1, description=\"Normalized temperature\")\n    atemp: float = Field(..., ge=0, le=1, description=\"Normalized feeling temperature\")\n    hum: float = Field(..., ge=0, le=1, description=\"Normalized humidity\")\n    windspeed: float = Field(..., ge=0, le=1, description=\"Normalized wind speed\")\n\n5    @field_validator(\"season\")\n6    def validate_season(cls, v):\n        allowed = {\"Winter\", \"Spring\", \"Summer\", \"Fall\"} \n7        if v not in allowed:\n8            raise ValueError(f\"season must be one of {allowed}\")\n        return v\n\n# -- snip -- (validators continue)\n\n\n1\n\nThe first thing to notice is that the Pydantic model is defined as a Python class that inherits from BaseModel; this is the Pydantic base class used to create models with built-in data validation.\n\n2\n\nEach class attribute is defined with a data type; season, for example, is defined as str. We also use the function Field() to specify constraints and metadata; the ellipsis literal ... here means two things: first, a value must be provided, second, there is no default value for this attribute. Then, some metadata is specified in the description parameter.\n\n3\n\nFor hr attribute, we have constraints used for numeric validation. So, ge=0 means that the hour has to be greater or equal to 0, while le=23 indicates that the hour should be less or equal to 23. So if the user send a request with hr=-1 the system will complain and throw a warning.\n\n4\n\nWith what we know so far, we could read this line as ‚Äúworkingday is a required str field with a description.‚Äù\n\n5\n\nWe have the field_validator decorator8 to build custom validation logic for a specific field. Besides the already fantastic default capabilities of Pydantic, we can define our custom validations. In this case, we want to be sure that the client will provide only valid values for the season field. CatBoost uses categories as predictors, and it is case sensitive (i.e., \"Summer\" != \"summer). With this extra step, FastAPI will send a custom message to the client, informing that the API expects \"Summer\" not \"summer\".\n\n6\n\nWe define the function validate_season, which takes two parameters: cls, a keyword indicating a reference to the class (BikeSharingRequest), and v, representing the value of the field we‚Äôre validating.\n\n7\n\nCheck if the value we validate is within the allowed set.\n\n8\n\nRaise an error if the value is not allowed; it also tells the client which values are expected.\n\n\n\n\nTo summarize the structure of our Pydantic model:\nFilename: app/models/bike_sharing.py\n\n1from pydantic import BaseModel, Field, field_validator\n\n2class BikeSharingRequest(BaseModel):\n3    season: str = Field(..., description=\"Season (Winter/Spring/Summer/Fall)\")\n    mnth: str = Field(..., description=\"Month name (January, etc.)\")\n    hr: int = Field(..., ge=0, le=23, description=\"Hour of the day\")\n    holiday: str = Field(..., description=\"Yes/No if holiday\")\n    weekday: str = Field(..., description=\"Name of the weekday (Monday/Tuesday/Wednesday/Thursday/Friday/Saturday/Sunday)\")\n    workingday: str = Field(..., description=\"Yes/No if working day\")\n    weathersit: int = Field(..., description=\"Weather code\")\n    temp: float = Field(..., ge=0, le=1, description=\"Normalized temperature\")\n    atemp: float = Field(..., ge=0, le=1, description=\"Normalized feeling temperature\")\n    hum: float = Field(..., ge=0, le=1, description=\"Normalized humidity\")\n    windspeed: float = Field(..., ge=0, le=1, description=\"Normalized wind speed\")\n\n4    @field_validator(\"season\")\n    def validate_season(cls, v):\n        allowed = {\"Winter\", \"Spring\", \"Summer\", \"Fall\"}\n        if v not in allowed:\n            raise ValueError(f\"season must be one of {allowed}\")\n        return v\n\n# -- snip -- (validators continue)\n\n\n1\n\nRequired Pydantic imports.\n\n2\n\nThe model is defined as a Python class.\n\n3\n\nWithin the class, we define the model‚Äôs fields as attributes of the class\n\n4\n\nWe can include custom validators within the model.\n\n\n\n\nGreat! Now, we have a Pydantic model and a better understanding of it. Next, we will work on our API.\n\n\nFastAPI development\nWith the data model in place (BikeSharingRequest), it‚Äôs time to develop the core of our FastAPI microservice. FastAPI makes it easy to build performant APIs with built-in data validation and clear, concise code. In this section, we‚Äôll walk through setting up the app, creating endpoints, and serving a machine learning model for predictions.\nTo get started with our FastAPI app, we create the Python file app/main.py. Let‚Äôs take a look at the imports we‚Äôre gonna need.\nFilename: app/main.py\n\nfrom fastapi import FastAPI\nfrom typing import List\nfrom app.services.ml_model import load_model\nfrom app.services.requests_to_polars import req_to_polars\n1from app.models.bike_sharing import BikeSharingRequest\nfrom fastapi import HTTPException\n\n# --snip--\n\n\n1\n\nIn this line we import the Pydantic data model we created in the previous step.\n\n\n\n\nNext, we will begin coding the core of our FastAPI app, starting with the instantiation of the main application object, app. This is done using the FastAPI() constructor. The annotated code is shown below.\nFilename: app/main.py\n\n# -- snip --\n\n# Instantiate a FastAPI class, the core of the application\n1app = FastAPI()\n\n# Read predictive model\n2ml_model = load_model('predictive_models/catboost_model_19Dec2024.cbm')\n\n# Defines a GET endpoin at the root path \"/\"\n3@app.get(\"/\")\nasync def index() -&gt; dict:\n    return {\"message\": \"Bike rentals ML predictor (regressor)\"} # Returns a simple JSON response with a message\n\n# --snip--\n\n\n1\n\nInstantiates the main FastAPI application. Every endpoint and configuration will attach to this app object.\n\n2\n\nRead the predictive model, which is a trained CatBoost model.\n\n3\n\nA GET endpoint to the root URL. When someone accesses this endpoint, they will be served with the ‚Äúmessage.‚Äù\n\n\n\n\nSo far, we have instantiated the FastAPI application and defined the root endpoint. Next, we‚Äôll implement the core functionality of our app by defining the /predict endpoint. This endpoint serves as the heart of the API, handling four key responsibilities:\n\nValidation: Ensures the incoming requests meet the expected format and constraints.\nData Processing: Transforms input data into a format compatible with the trained CatBoost model.\nPrediction: Uses the model to generate predictions based on the input.\nResponse Generation: Packages the predictions into a JSON response to return to the client.\n\n\nDefining the /predict Endpoint\nIn this step, we define the core functionality of our API: the /predict endpoint. This endpoint handles incoming POST requests, validates the input data, processes it, generates predictions using the trained CatBoost model, and returns the predictions as a JSON response.\nLet‚Äôs see the code.\nFilename: app/main.py\n\n# -- snip --\n\n# Defines a POST endpoint for predictions\n1@app.post('/predict',\n          summary = \"Predict bike rentals\",\n          description = \"Takes input data and returns amount rental predictions (regression).\")\n2async def predict_rentals(requests: List[BikeSharingRequest]) -&gt; dict:\n    \"\"\"\n    Predict bike rentals based on input features.\n\n    Args:\n        requests (List[BikeSharingRequests]): A list of input data objects.\n\n    Returns:\n        dict: A dictionary containing the predictions as a list.\n    \"\"\"\n\n    # Handles the case when an empty list is posted\n3    if not requests:\n        raise HTTPException(\n            status_code = 422,\n            detail = \"The request is empty\"\n        )\n    \n    # Convert each pydantic model to a dict and load into a Polars DataFrame\n4    df = req_to_polars(requests)\n\n    # Instances to predict\n5    features = df.to_numpy()\n\n    # Make predictions\n6    predictions = ml_model.predict(features)\n\n    return {\n7        'predictions': predictions.tolist()\n    }\n\n\n1\n\nPOST Endpoint Definition: Defines a POST endpoint at /predict with metadata for better documentation (summary and description).\n\n2\n\nInput Validation: Expects a list of BikeSharingRequest objects, ensuring automatic data validation by FastAPI.\n\n3\n\nEmpty Request Handling: If the request body is empty ([]), returns a 422 Unprocessable Entity error with a descriptive message.\n\n4\n\nData Transformation: Converts the list of validated Pydantic objects into a Polars DataFrame for efficient processing.\n\n5\n\nCompatibility with CatBoost: Since CatBoost does not accept Polars DataFrames, the data is converted to a NumPy array.\n\n6\n\nPredictions: Uses the trained CatBoost model to compute predictions based on the input data.\n\n7\n\nResponse Creation: Returns the predictions as a JSON-compatible dictionary.\n\n\n\n\n\n\nEndpoint Responsibilities\nThe predict_rentals function handles multiple responsibilities:\n\nEdge Case Handling: Validates and handles empty input.\nData Transformation: Converts structured input data into a format compatible with the machine learning model (numpy).\nInference: Generates predictions using the trained CatBoost model.\nResponse Generation: Packages the results into a JSON response.\n\n\n\nTesting the Application\nWith the ‚Äò/predict‚Äô endpoint defined, we now have the core functionality of our prediction API. Let‚Äôs test it!\n\nClone the Repository\nIf you haven‚Äôt already, clone the GitHub repository to follow along.\n\n\nRun the Application\nFrom the root of the project, start the FastAPI app using Uvicorn:\nuvicorn app.main:app --host 127.0.0.1 --port 8000\nOnce the application is running, you should see output similar to this in your terminal:\nuser@user-pc:~/ml_model_api$ uvicorn app.main:app\nINFO:     Started server process [19168]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n\n\n\nExplore the API\nWith the app running, you can now access the API:\n\nAPI Root: http://127.0.0.1:8000/\n\nWhen you visit the root endpoint, you‚Äôll see the following message:\n\n{\n  \"message\": \"Bike rentals ML predictor (regressor)\"\n}\n\n\n\n\nPydantic model\nOne of the standout features of FastAPI is its ability to automatically generate API documentation, complete with interactive functionality. You can access the API documentation for this project by visiting:\n\nSwagger UI (API Documentation): http://127.0.0.1:8000/docs\n\nIn this interface, you can:\n\nExplore the API‚Äôs endpoints and their specifications.\nTest the endpoints directly using the provided interface.\nExamine the Pydantic model used for data validation.\n\n\nExploring the Pydantic Model\nFastAPI uses Pydantic under the hood to validate and parse incoming JSON data. If you navigate to the Swagger UI, you‚Äôll find the schema for the BikeSharingRequest model. This schema provides key information:\n\nField Descriptions: Attributes in the BikeSharingRequest class are documented, including the description parameter provided via Field().\nData Type Constraints: Default type constraints (e.g., string, integer) are visible for each field.\nRange Restrictions: For numeric fields, restrictions such as [0, 23] for the hr field are displayed clearly in square brackets.\n\n\n\nCustom Validators\nWhile custom validators are not automatically displayed in the Swagger UI, their logic is included (manually) as metadata in the description of the corresponding fields. For instance, you can view notes about specific constraints or requirements that are enforced programmatically.\nFastAPI ensures each JSON object in the incoming list is validated and parsed into a BikeSharingRequest instance using Pydantic. This guarantees data consistency and simplifies further processing.\n\n\n\nSwagger UI. The Pydantic model for BikeSharingRequest is expanded in the Schemas section\n\n\n\n\n\nFastAPI app\nThe FastAPI process begins when a user sends a POST request containing input data in JSON format. FastAPI inspects the request body and knows to parse it as a List[BikeSharingRequest] object. FastAPI knows how to parse the JSON due to the endpoint function signature:\nFilename: app/main.py\n\n@app.post('/predict', \n          summary = \"Predict bike rentals\", \n          description = \"Takes input data and returns amount rental predictions (regression).\")\n1async def predict_rentals(requests: List[BikeSharingRequest]) -&gt; dict:\n\n\n1\n\nIn the function signature, the List[BikeSharingRequest] annotation informs FastAPI to interpret the raw JSON input as a list of JSON objects that match the BikeSharingRequest schema.\n\n\n\n\n\nProcess Breakdown\n\n1. JSON Parsing and Validation\n\nFastAPI loops through the JSON objects in the request body.\nEach JSON object is validated against the BikeSharingRequest Pydantic model.\nValidation includes:\n\nEnsuring all required fields are present.\nVerifying field types and constraints.\nApplying custom validators defined in the BikeSharingRequest model.\n\n\n\n\n2. Error Handling\n\nIf any JSON object fails validation, FastAPI stops the process and returns a 422 Unprocessable Entity response, including details about the validation errors.\nThe predict_rentals function is not called if validation fails.\n\n\n\n3. Object Creation\n\nIf all JSON objects pass validation, FastAPI creates a list of BikeSharingRequest instances and passes them to the predict_rentals function as the requests argument.\n\n\n\n4. Data Shaping and Inference\n\nInside the predict_rentals function:\n\nThe input data is shaped for inference by the CatBoost model.\nFrom List(BikeSharingRequest) to numpy array.\nPredictions are generated using the trained model.\n\n\n\n\n5. Response\n\nThe predictions are formatted as a JSON response and sent back to the user.\n\n\n\n\nVisual Representation\nThe entire process is summarized in the flowchart below:\n\n\n\nVisual representation of our FastAPI app\n\n\nFor simplicity, the Uvicorn server is abstracted away from the diagram. The focus is solely on the core FastAPI application flow, from receiving a request to sending a response.\nWith the core functionality of our FastAPI application in place, (i.e., the `/predict/predict endpoint for serving predictions), it‚Äôs time to focus on deployment and automation. While building a robust application is essential, ensuring that it can be reliably tested, built, and deployed is equally important. This is where Continuous Integration (CI) and Continuous Deployment (CD) come into play.\n\nBy implementing CI/CD pipelines, we can:\n\nAutomate Testing: Ensure every change to the codebase is verified by running unit tests and integration tests automatically.\nStreamline Deployment: Build and push a Docker image of the application to a container registry (e.g., Docker Hub), ready for deployment in any environment.\nMaintain Code Quality: Catch bugs early and enforce consistent standards with every change.\n\nIn the next section, we‚Äôll set up a CI/CD workflow using GitHub Actions to:\n\nRun automated tests with pytest.\nBuild and publish a Docker image of our FastAPI app.\nLet‚Äôs dive into automating the lifecycle of our application!"
  },
  {
    "objectID": "bikesharing_api.html#automated-tests",
    "href": "bikesharing_api.html#automated-tests",
    "title": "Bike Rentals Prediction API",
    "section": "Automated tests",
    "text": "Automated tests"
  },
  {
    "objectID": "bikesharing_api.html#containarization",
    "href": "bikesharing_api.html#containarization",
    "title": "Bike Rentals Prediction API",
    "section": "Containarization",
    "text": "Containarization"
  },
  {
    "objectID": "bikesharing_api.html#inference",
    "href": "bikesharing_api.html#inference",
    "title": "Bike Rentals Prediction API",
    "section": "Inference",
    "text": "Inference"
  },
  {
    "objectID": "bikesharing_api.html#microservice-up-and-running",
    "href": "bikesharing_api.html#microservice-up-and-running",
    "title": "Bike Rentals Prediction API",
    "section": "Microservice up and running",
    "text": "Microservice up and running"
  },
  {
    "objectID": "bikesharing_api.html#what-we-learned",
    "href": "bikesharing_api.html#what-we-learned",
    "title": "Bike Rentals Prediction API",
    "section": "What we learned?",
    "text": "What we learned?"
  },
  {
    "objectID": "bikesharing_api.html#whats-left",
    "href": "bikesharing_api.html#whats-left",
    "title": "Bike Rentals Prediction API",
    "section": "Whats left?",
    "text": "Whats left?\nModel monitoring, automatic training incorporated to the CI/CD, model versioning, high level applications that consume the APIs"
  },
  {
    "objectID": "bikesharing_api.html#recommended-readings",
    "href": "bikesharing_api.html#recommended-readings",
    "title": "Bike Rentals Prediction API",
    "section": "Recommended readings",
    "text": "Recommended readings\n\nMicroservice APIs by Jose Haro Peralta\nIf you‚Äôre new to APIs and microservices, Microservice APIs by Jose Haro Peralta provides an excellent starting point. The author presents fundamental software architecture and engineering principles in an accessible, hands-on way. You‚Äôll find practical code examples to experiment with, paired with clear theoretical explanations that help you understand both the ‚Äúhow‚Äù and the ‚Äúwhy‚Äù behind the code.\n\n\nDocker Deep Dive by Nigel Poulton\nFor anyone looking to learn Docker, Nigel Poulton‚Äôs Docker Deep Dive offers a concise yet insightful introduction. Despite its brevity, the book covers many concepts and provides straightforward exercises. By following along with the examples, you‚Äôll quickly gain the confidence to use Docker effectively in real-world scenarios.\n\n\nSoftware Engineering for Data Scientists by Andrew Treadway\nCurrently available as a MEAP (Manning Early Access Program), Software Engineering for Data Scientists by Andrew Treadway is a remarkable resource for data professionals eager to strengthen their software engineering skills. The book‚Äôs in-progress chapters seamlessly blend theory and practice, offering well-researched explanations and practical examples. This is a must-read if you want to elevate your data projects with solid engineering principles."
  },
  {
    "objectID": "bikesharing_api.html#about-the-pondering-guy-in-the-header",
    "href": "bikesharing_api.html#about-the-pondering-guy-in-the-header",
    "title": "Bike Rentals Prediction API",
    "section": "About the pondering guy in the header",
    "text": "About the pondering guy in the header\nIt‚Äôs the wizard from ‚ÄúPondering My Orb.‚Äù The art is the work by Angus Mcbride and was featured in a Lord of the Rings game book :) you can check Know Your Meme for more info/memes."
  },
  {
    "objectID": "bikesharing_api.html#footnotes",
    "href": "bikesharing_api.html#footnotes",
    "title": "Bike Rentals Prediction API",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis must be how George R. R. Martin feels when foreshadowing a major (traumatic) event in his novels.‚Ü©Ô∏é\nAt the time of writing, I am aware that ‚Äúno‚Äù is the answer since we‚Äôre still just at the introduction of this tutorial.‚Ü©Ô∏é\nIt‚Äôs even worse today, in our modern remote era. The dreaded Microsoft Teams ringtone will constantly haunt you.‚Ü©Ô∏é\nYou probably already noticed that we keep bringing up the link to this notebook. That‚Äôs because we spent so much time on it, and we was hoping you could look at our precious.‚Ü©Ô∏é\nI learned this tip for technical writing from ‚ÄúThe Rust Programming Language‚Äù book by Steve Klabnik, Carol Nichols, and the Rust Community. The book is a masterpiece and a highly recommended reading; you can find it here.‚Ü©Ô∏é\nWhoops, here is the notebook again. When presenting results to my stakeholders as a habit, I always prepare two decks, one with very high-level results and a second highly technical one with all the math of the statistical/ML models and intricacies of the system design, just in case somebody has a question on the interesting technical part. This is expecting a deep, nerdy discussion about models, gaps, opportunities, and science; to date, the counts for the times I‚Äôve used this second deck remain at 0. The good news is that it might be a proxy of our stakeholders‚Äô trust in us.‚Ü©Ô∏é\n„Ç®„É≥„ÉÅ„É©„Éº„ÉÄ„Çí„ÅÑ„Åü„Å†„Åë„Åæ„Åô„ÅãÔºü‚Ü©Ô∏é\nA decorator is a function whose intention is to modify the behavior of other functions; it takes the original function and converts it into a different function. Luciano Ramalho does a great job explaining decorators in chapter 9 of his book ‚ÄúFluent Python‚Äù.‚Ü©Ô∏é"
  },
  {
    "objectID": "bikesharing_api.html#cicd",
    "href": "bikesharing_api.html#cicd",
    "title": "Bike Rentals Prediction API",
    "section": "CI/CD",
    "text": "CI/CD\nContinuous Integration (CI) and Continuous Deployment (CD)"
  },
  {
    "objectID": "bikesharing_api.html#citation",
    "href": "bikesharing_api.html#citation",
    "title": "Bike Rentals Prediction API",
    "section": "Citation",
    "text": "Citation\nNo needed, but feel free to mention me üòÑ I would appreciate it a lot!"
  },
  {
    "objectID": "bikesharing_api.html#cicd-continuous-integration-continuos-deployment",
    "href": "bikesharing_api.html#cicd-continuous-integration-continuos-deployment",
    "title": "Bike Rentals Prediction API",
    "section": "CI/CD (Continuous Integration / Continuos Deployment)",
    "text": "CI/CD (Continuous Integration / Continuos Deployment)\nContinuous Integration (CI) and Continuous Deployment (CD) are core to any software development system. I would love to be the one who gives you a great practical introduction here, but that one will be Itskmyoo, which already writed an great (and simple!) tutorial to automate with GitHub Actions the CI/CD your FastAPI developments.\nIn his/her tutoria Itskmyoo, provides a clear set of instructions to setup GitHub Actions, and does it very well! The only modification we will do is in our Dockerfile. Its contents should look like:\n\n# Use an official Python runtime as a parent image\nFROM python:3.12-slim\n\n# Set the working directory in the container\nWORKDIR /api_app\n\n# Copy the requirements file into the container at /app\nCOPY requirements.txt /api_app/\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the current directory contents into the container at /app\nCOPY . /api_app/\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Define environment variable\nENV NAME=ml-model-api-docker\n\n# Set the maintainer label\nLABEL maintainer=\"jospablo777 &lt;jospablo777@gmail.com&gt;\"\n\n# Run main.py when the container launches\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]"
  }
]