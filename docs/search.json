[
  {
    "objectID": "bikesharing_api.html#an-accesible-inference-system",
    "href": "bikesharing_api.html#an-accesible-inference-system",
    "title": "Bike Rentals Prediction API",
    "section": "An accesible inference system",
    "text": "An accesible inference system\nIn a fast-paced world, one of the problems we face as data professionals is delivering our data products promptly. Also, distributing them in a manner that doesn’t affect our productivity, for example, manually running a Jupyter Notebook or an R script every time someone needs insights, is awfully unproductive.\nAn idea that is not new, is to deliver the “answers” in an automated way, from which the clients can self-serve each time they are in need. After we have a data product, for instance, a predictive model (machine learning if you are into buzzwords), we can make available its capabilities to our stakeholders via a micro service.\nWith that in mind, we will use this small tutorial to understand better the building of a production-ready machine learning microservice to predict bike rentals. By the end, you’ll have a better understanding of using FastAPI for API development, Docker for containerization, and GitHub Actions for CI/CD. Whether you’re a data scientist looking to scale your models or a developer exploring microservices, this guide is for you. To take better advantage of this guide, please keep the accompanying GitHub repo at hand."
  },
  {
    "objectID": "bikesharing_api.html#an-ml-model-as-a-microservice",
    "href": "bikesharing_api.html#an-ml-model-as-a-microservice",
    "title": "Bike Rentals Prediction API",
    "section": "An ML model as a microservice",
    "text": "An ML model as a microservice\nImagine your (predictive) model being accessed anytime without disturbing your peace or consuming your time; after all, your time is expensive, and you have many tasks. Here is where the idea of microservices comes into action. A microservice is an isolated piece of software that is in charge of a single task (service) and communicates through a well-defined API. You finish your model, pack it in a microservice, and make it available through an API.\nThis way, the model will always be available, and its consumption won’t block other tasks. Also, more products could be potentially developed with the API."
  },
  {
    "objectID": "bikesharing_api.html#but-what-is-an-api",
    "href": "bikesharing_api.html#but-what-is-an-api",
    "title": "Bike Rentals Prediction API",
    "section": "But what is an API?",
    "text": "But what is an API?\nAn API is an intermediary that enables you to interact with a service without requiring knowledge of how this service works. You just need to know precisely what you want, tell it to the API, and the API will serve you what you requested.\nLet’s say that after work, you get hungry, so you park your Capital Bikeshare™ bike out of your favorite Mexican restaurant1. You enter the restaurant and sit at a table. Instead of going directly to the kitchen to cook your own meal or asking the chef what you want, you need to place your order through the waiter or waitress. You kindly request your enchiladas, and after a while, they bring the meal to you.\n\n\n\nAllegory of a restaurant. The waiter/waitress is the API; the kitchen is the service (they are dedicated to cooking); you are the client\n\n\nNotice that you, as the client, are not required to know how to cook enchiladas. Also, you do not interact directly with the people who prepare the food. You get what you want through a waiter (the API).\nIn summary, an API is the piece of the system you interact with to get a result generated by a service. Producing this result might be complex, but you don’t care about that since you only have to deal with the API."
  },
  {
    "objectID": "bikesharing_api.html#software-engineering-for-data-scientists",
    "href": "bikesharing_api.html#software-engineering-for-data-scientists",
    "title": "Bike Rentals Prediction API",
    "section": "Software engineering for data scientists",
    "text": "Software engineering for data scientists\nAnother problem common to data scientists is that research, predictive modeling, and analytics are dirty processes. This is partly due to a lack of training in software engineering, which involves design, testing, and software maintenance.\nWe usually have many problems to solve, questions to answer, and limited time at work. This leaves little to no time to apply the best practices to develop our analysis and maintain our code, leaving a trail of technical debt with each delivery. Yet, since most of us write code, we are, in fact, software developers because we develop software. This should be a hard-to-miss hint that software engineering practices must be core in our profession, or at least not ignored.\nIn this hands-on project, we will implement some practices core of software engineering like:\n\nCI/CD: Continuous Integration (CI) and Continuous Deployment (CD) with GitHub Actions.\nAutomated tests: unit, integration, and inference tests with pytest.\nContainerization: package the application with Docker for consistency across environments (i.e., it should work on every machine with a well-setup Docker).\nDependency Management: For reproducibility, dependencies were specified in a requirements.txt file, and development was conducted in an isolated environment (venv).\nMicroservices architecture: The project is a self-contained microservice focused on a single responsibility: predicting bike rentals. It is independently deployable and communicates through HTTP.\n\nWe intend to review these concepts in a practical, applied way. This will give us more tools to design and produce more sustainable systems, which we can develop more efficiently through good practices without sacrificing smooth delivery to our clients."
  },
  {
    "objectID": "bikesharing_api.html#about-the-technologies",
    "href": "bikesharing_api.html#about-the-technologies",
    "title": "Bike Rentals Prediction API",
    "section": "About the technologies",
    "text": "About the technologies\nThrough this practical project, we will deal with several technologies, the most important:\n\nFastAPI: a framework to develop APIs with Python. It is easy to learn, and it makes the development of APIs incredibly fast.\nUvicorn: an ASGI (Asynchronous Server Getaway Interface) Server. It handles incoming HTTP requests and sends responses. This lightweight server is appropriate for our microservice.\nDocker: a platform to pack our application into isolated “containers” and make them available to be run in several systems. Perfect to build microservices.\nGitHub Actions: a CI/CD platform that allows us to automate your development pipelines. We will use them here to automate the software tests and build and push our docker images.\npytest: a Python framework to simplify our software testing.\n\nThese tools will make the development and publishing of our microservice a breeze."
  },
  {
    "objectID": "bikesharing_api.html#lets-get-started",
    "href": "bikesharing_api.html#lets-get-started",
    "title": "Bike Rentals Prediction API",
    "section": "Let’s get started",
    "text": "Let’s get started\nWe will start with the mandatory xkcd comic that this kind of articles include so we can continue with the tutorial.\n\n\n\nExploits of a Mom, by xkcd Comics. Available at https://xkcd.com/3027/\n\n\nI know that this has nothing to do with software engineering or APIs, but little Bobby Tables always cracks me laugh. Now we can continue with the build of our API and some software engineering practices."
  },
  {
    "objectID": "bikesharing_api.html#problem-we-want-to-solve",
    "href": "bikesharing_api.html#problem-we-want-to-solve",
    "title": "Bike Rentals Prediction API",
    "section": "Problem we want to solve",
    "text": "Problem we want to solve\nNow, let’s focus on the problem we want to solve; this is probably the most crucial step since it is how we add value to the business. We are hired to solve analytical problems with evidence (i.e., data) as our raw material to produce those solutions.\nYou won’t be surprised to learn this project started as a Jupyter Notebook, which I encourage you to check out here. We are using an open data set from the UCI Machine Learning Repository.\nLet’s assume we are working for Capital Bikeshare, a company dedicated to providing a self-service bike rental service managed through a mobile app. For the company, it is crucial to predict the peaks of service usage so they can restock or make more available their products when they’re more needed. These products can be cloud computing resources during peak hours, bicycle units, human staff, etc. Also it can be convenient to predict the dates and hours of least use for maintenance tasks, like server updates or bike maintenance.\n\n\n\nCapital Bikeshare station outside of the Eastern Market Metro station. Author: Ben Schumin (2010). Licensed under CC BY-SA 3.0\n\n\nWorking as a data scientist for a big tech transport company, you will have lots of data and computational resources. On this occasion, you were put in charge of finding a way to predict the days and hours they will have to replenish their stock and increase the server’s capacity, investing in more resources only when necessary (saving money). Also, knowing when to schedule the maintenance labor will be valuable as well.\nAfter finishing the data exploration, you know that working days and the entry and exit from work hours are the busiest time frames. Also, climatic conditions can affect the willingness to rent a bike. You are cautious, so you first took the time to understand the phenomenon. Then, you built the model prototype and corroborated that it is possible to forecast the number of rented bikes at an hour-of-the-day granularity level.\nSuccess! You have a model that can help determine the best time to provide more resources or the times for maintenance. Does this mean that you have already finished with the task and can go on to live the good life?2 Well, you already know that Jupyter is not a tool that produces clean results and that if you leave it there, your model will have to be run manually each time, which can introduce bugs and errors. This also means that you will have a high and constant influx of people knocking at your door3 asking for predictions and insights.\nJupyter is ideal for experimenting and testing but not for delivering results. Hence, you have decided to deploy this first iteration of the model (the prototype) as a microservice that can be consumed via an API. Here, you can assess how the model behaves in the wild and prepare for future iterations. You will start adding value with predictions and valuable information that will help improve your model.\nThe engineering team, for example, will quickly adopt the app to know when to conduct maintenance jobs."
  },
  {
    "objectID": "bikesharing_api.html#microservice-development",
    "href": "bikesharing_api.html#microservice-development",
    "title": "Bike Rentals Prediction API",
    "section": "Microservice development",
    "text": "Microservice development\nWe were assigned a task, so we started by investigating bike rental data and then continued by building a predictive model using the CatBoost algorithm. This model will be the inference engine of our microservice. You can see this part of the project here in the notebook4. We saved the model in our project’s predictive_models/ folder.\n\n\n\n\n\n\nNote\n\n\n\nDuring this tutorial, we will show you some code snippets and indicate the file where the code is located5. You should then, be able to follow the project’s code, which is available on the GitHub repo.\n\n\nAfter some research and experimentation, we developed a CatBoost model capable of predicting the number of rented bikes at a specific time, given some environmental and temporal conditions. The code that generated the model is next.\nFilename: notebooks/eda_and_toy_model.ipynb\n\nfrom catboost import CatBoostRegressor\n\n# --snip--\n\n# Instantiate our model\ncatboost_model = CatBoostRegressor(\n    iterations=1000,       # Boosting iterations\n    learning_rate=0.1,\n    depth=6,               # Tree depth\n    loss_function='RMSE',  # Loss\n    verbose=100            # Let it print the learning state every 100 iterations\n)\n\n# Train\ncatboost_model.fit(X_train, y_train, cat_features=categorical_features)\n\n# --snip--\n\n# Save trained model\ncatboost_model.save_model(\"../predictive_models/catboost_model_19Dec2024.cbm\")\n\nIn this analysis, we generated some insights6 and the core of our microservice, a predictive model stored in predictive_models/catboost_model_19Dec2024.cbm. Now that we have our predictor, let’s see how we can make the predictions accessible to others in the company."
  },
  {
    "objectID": "bikesharing_api.html#api-development",
    "href": "bikesharing_api.html#api-development",
    "title": "Bike Rentals Prediction API",
    "section": "API development",
    "text": "API development\nThe API will move data around (receive and deliver) as its primary job, so it is key to establish a clear set of rules for the data our clients send. Moving back to our restaurant example, you probably could put your enchiladas order in English or Spanish, but it might be hard for the waiter if you place the order in Japanese7. Hence, to get your enchiladas, you must request them in a way that the staff can understand, and Pydantic is here to enforce the rules that make such understanding happen.\n\n\n\nAn ambiguous (and invalid) request to the API, so no enchiladas :(\n\n\n\nDefining the Pydantic model\nWe will start this section by defining our Pydantic model. Pydantic is a library for data validation that uses Python-type annotations. Let’s establish a Pydantic model for our API requests and learn more about the library in the process.\nFilename: app/models/bike_sharing.py\n\nfrom pydantic import BaseModel, Field, field_validator\n\n1class BikeSharingRequest(BaseModel):\n2    season: str = Field(..., description=\"Season (Winter/Spring/Summer/Fall)\")\n    mnth: str = Field(..., description=\"Month name (January, etc.)\")\n3    hr: int = Field(..., ge=0, le=23, description=\"Hour of the day\")\n    holiday: str = Field(..., description=\"Yes/No if holiday\")\n    weekday: str = Field(..., description=\"Name of the weekday (Monday/Tuesday/Wednesday/Thursday/Friday/Saturday/Sunday)\")\n4    workingday: str = Field(..., description=\"Yes/No if working day\")\n    weathersit: int = Field(..., description=\"Weather code\")\n    temp: float = Field(..., ge=0, le=1, description=\"Normalized temperature\")\n    atemp: float = Field(..., ge=0, le=1, description=\"Normalized feeling temperature\")\n    hum: float = Field(..., ge=0, le=1, description=\"Normalized humidity\")\n    windspeed: float = Field(..., ge=0, le=1, description=\"Normalized wind speed\")\n\n5    @field_validator(\"season\")\n6    def validate_season(cls, v):\n        allowed = {\"Winter\", \"Spring\", \"Summer\", \"Fall\"} \n7        if v not in allowed:\n8            raise ValueError(f\"season must be one of {allowed}\")\n        return v\n\n# -- snip -- (validators continue)\n\n\n1\n\nThe first thing to notice is that the Pydantic model is defined as a Python class that inherits from BaseModel; this is the Pydantic base class used to create models with built-in data validation.\n\n2\n\nEach class attribute is defined with a data type; season, for example, is defined as str. We also use the function Field() to specify constraints and metadata; the ellipsis literal ... here means two things: first, a value must be provided, second, there is no default value for this attribute. Then, some metadata is specified in the description parameter.\n\n3\n\nFor hr attribute, we have constraints used for numeric validation. So, ge=0 means that the hour has to be greater or equal to 0, while le=23 indicates that the hour should be less or equal to 23. So if the user send a request with hr=-1 the system will complain and throw a warning.\n\n4\n\nWith what we know so far, we could read this line as “workingday is a required str field with a description.”\n\n5\n\nWe have the field_validator decorator8 to build custom validation logic for a specific field. Besides the already fantastic default capabilities of Pydantic, we can define our custom validations. In this case, we want to be sure that the client will provide only valid values for the season field. CatBoost uses categories as predictors, and it is case sensitive (i.e., \"Summer\" != \"summer). With this extra step, FastAPI will send a custom message to the client, informing that the API expects \"Summer\" not \"summer\".\n\n6\n\nWe define the function validate_season, which takes two parameters: cls, a keyword indicating a reference to the class (BikeSharingRequest), and v, representing the value of the field we’re validating.\n\n7\n\nCheck if the value we validate is within the allowed set.\n\n8\n\nRaise an error if the value is not allowed; it also tells the client which values are expected.\n\n\n\n\nTo summarize the structure of our Pydantic model:\nFilename: app/models/bike_sharing.py\n\n1from pydantic import BaseModel, Field, field_validator\n\n2class BikeSharingRequest(BaseModel):\n3    season: str = Field(..., description=\"Season (Winter/Spring/Summer/Fall)\")\n    mnth: str = Field(..., description=\"Month name (January, etc.)\")\n    hr: int = Field(..., ge=0, le=23, description=\"Hour of the day\")\n    holiday: str = Field(..., description=\"Yes/No if holiday\")\n    weekday: str = Field(..., description=\"Name of the weekday (Monday/Tuesday/Wednesday/Thursday/Friday/Saturday/Sunday)\")\n    workingday: str = Field(..., description=\"Yes/No if working day\")\n    weathersit: int = Field(..., description=\"Weather code\")\n    temp: float = Field(..., ge=0, le=1, description=\"Normalized temperature\")\n    atemp: float = Field(..., ge=0, le=1, description=\"Normalized feeling temperature\")\n    hum: float = Field(..., ge=0, le=1, description=\"Normalized humidity\")\n    windspeed: float = Field(..., ge=0, le=1, description=\"Normalized wind speed\")\n\n4    @field_validator(\"season\")\n    def validate_season(cls, v):\n        allowed = {\"Winter\", \"Spring\", \"Summer\", \"Fall\"}\n        if v not in allowed:\n            raise ValueError(f\"season must be one of {allowed}\")\n        return v\n\n# -- snip -- (validators continue)\n\n\n1\n\nRequired Pydantic imports.\n\n2\n\nThe model is defined as a Python class.\n\n3\n\nWithin the class, we define the model’s fields as attributes of the class\n\n4\n\nWe can include custom validators within the model.\n\n\n\n\nGreat! Now, we have a Pydantic model and a better understanding of it. Next, we will work on our API.\n\n\nFastAPI development\nWith the data model in place (BikeSharingRequest), it’s time to develop the core of our FastAPI microservice. FastAPI makes it easy to build performant APIs with built-in data validation and clear, concise code. In this section, we’ll walk through setting up the app, creating endpoints, and serving a machine learning model for predictions.\nTo get started with our FastAPI app, we create the Python file app/main.py. Let’s take a look at the imports we’re gonna need.\nFilename: app/main.py\n\nfrom fastapi import FastAPI\nfrom typing import List\nfrom app.services.ml_model import load_model\nfrom app.services.requests_to_polars import req_to_polars\n1from app.models.bike_sharing import BikeSharingRequest\nfrom fastapi import HTTPException\n\n# --snip--\n\n\n1\n\nIn this line we import the Pydantic data model we created in the previous step.\n\n\n\n\nNext, we will begin coding the core of our FastAPI app, starting with the instantiation of the main application object, app. This is done using the FastAPI() constructor. The annotated code is shown below.\nFilename: app/main.py\n\n# -- snip --\n\n# Instantiate a FastAPI class, the core of the application\n1app = FastAPI()\n\n# Read predictive model\n2ml_model = load_model('predictive_models/catboost_model_19Dec2024.cbm')\n\n# Defines a GET endpoin at the root path \"/\"\n3@app.get(\"/\")\nasync def index() -&gt; dict:\n    return {\"message\": \"Bike rentals ML predictor (regressor)\"} # Returns a simple JSON response with a message\n\n# --snip--\n\n\n1\n\nInstantiates the main FastAPI application. Every endpoint and configuration will attach to this app object.\n\n2\n\nRead the predictive model, which is a trained CatBoost model.\n\n3\n\nA GET endpoint to the root URL. When someone accesses this endpoint, they will be served with the “message.”\n\n\n\n\nSo far, we have instantiated the FastAPI application and defined the root endpoint. Next, we’ll implement the core functionality of our app by defining the /predict endpoint. This endpoint serves as the heart of the API, handling four key responsibilities:\n\nValidation: ensures the incoming requests meet the expected format and constraints.\nData Processing: transforms input data into a format compatible with the trained CatBoost model.\nPrediction: uses the model to generate predictions based on the input.\nResponse Generation: uackages the predictions into a JSON response to return to the client.\n\n\nDefining the /predict Endpoint\nIn this step, we define the core functionality of our API: the /predict endpoint. This endpoint handles incoming POST requests, validates the input data, processes it, generates predictions using the trained CatBoost model, and returns the predictions as a JSON response.\nLet’s see the code.\nFilename: app/main.py\n\n# -- snip --\n\n# Defines a POST endpoint for predictions\n1@app.post('/predict',\n          summary = \"Predict bike rentals\",\n          description = \"Takes input data and returns amount rental predictions (regression).\")\n2async def predict_rentals(requests: List[BikeSharingRequest]) -&gt; dict:\n    \"\"\"\n    Predict bike rentals based on input features.\n\n    Args:\n        requests (List[BikeSharingRequests]): A list of input data objects.\n\n    Returns:\n        dict: A dictionary containing the predictions as a list.\n    \"\"\"\n\n    # Handles the case when an empty list is posted\n3    if not requests:\n        raise HTTPException(\n            status_code = 422,\n            detail = \"The request is empty\"\n        )\n    \n    # Convert each pydantic model to a dict and load into a Polars DataFrame\n4    df = req_to_polars(requests)\n\n    # Instances to predict\n5    features = df.to_numpy()\n\n    # Make predictions\n6    predictions = ml_model.predict(features)\n\n    return {\n7        'predictions': predictions.tolist()\n    }\n\n\n1\n\nPOST Endpoint Definition: defines a POST endpoint at /predict with metadata for better documentation (summary and description).\n\n2\n\nInput Validation: expects a list of BikeSharingRequest objects, ensuring automatic data validation by FastAPI.\n\n3\n\nEmpty Request Handling: if the request body is empty ([]), returns a 422 Unprocessable Entity error with a descriptive message.\n\n4\n\nData Transformation: converts the list of validated Pydantic objects into a Polars DataFrame for efficient processing.\n\n5\n\nCompatibility with CatBoost: since CatBoost does not accept Polars DataFrames, the data is converted to a NumPy array.\n\n6\n\nPredictions: uses the trained CatBoost model to compute predictions based on the input data.\n\n7\n\nResponse Creation: returns the predictions as a JSON-compatible dictionary.\n\n\n\n\n\n\nEndpoint Responsibilities\nThe predict_rentals function handles multiple responsibilities:\n\nEdge Case Handling: validates and handles empty input.\nData Transformation: converts structured input data into a format compatible with the machine learning model (numpy).\nInference: generates predictions using the trained CatBoost model.\nResponse Generation: packages the results into a JSON response.\n\n\n\nTesting the Application\nWith the /predict endpoint defined, we now have the core functionality of our prediction API. Let’s test it!\n\nClone the Repository\nIf you haven’t already, clone the GitHub repository to follow along.\n\n\nRun the Application\nFrom the root of the project, start the FastAPI app using Uvicorn:\n\nuvicorn app.main:app --host 127.0.0.1 --port 8000\n\nOnce the application is running, you should see output similar to this in your terminal:\n\nuser@user-pc:~/ml_model_api$ uvicorn app.main:app\nINFO:     Started server process [19168]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n\n\n\n\nExplore the API\nWith the app running, you can now access the API:\n\nAPI Root: http://127.0.0.1:8000/\n\nWhen you visit the root endpoint, you’ll see the following message:\n\n{\n  \"message\": \"Bike rentals ML predictor (regressor)\"\n}\n\n\n\n\nPydantic model\nOne of the standout features of FastAPI is its ability to automatically generate API documentation, complete with interactive functionality. You can access the API documentation for this project by visiting:\n\nSwagger UI (API Documentation): http://127.0.0.1:8000/docs\n\nIn this interface, you can:\n\nExplore the API’s endpoints and their specifications.\nTest the endpoints directly using the provided interface.\nExamine the Pydantic model used for data validation.\n\n\nExploring the Pydantic Model\nFastAPI uses Pydantic under the hood to validate and parse incoming JSON data. If you navigate to the Swagger UI, you’ll find the schema for the BikeSharingRequest model. This schema provides key information:\n\nField Descriptions: attributes in the BikeSharingRequest class are documented, including the description parameter provided via Field().\nData Type Constraints: default type constraints (e.g., string, integer) are visible for each field.\nRange Restrictions: for numeric fields, restrictions such as [0, 23] for the hr field are displayed clearly in square brackets.\n\n\n\nCustom Validators\nWhile custom validators are not automatically displayed in the Swagger UI, their logic is included (manually) as metadata in the description of the corresponding fields. For instance, you can view notes about specific constraints or requirements that are enforced programmatically.\nFastAPI ensures each JSON object in the incoming list is validated and parsed into a BikeSharingRequest instance using Pydantic. This guarantees data consistency and simplifies further processing.\n\n\n\nSwagger UI. The Pydantic model for BikeSharingRequest is expanded in the Schemas section.\n\n\n\n\n\nFastAPI app\nThe FastAPI process begins when a user sends a POST request containing input data in JSON format. FastAPI inspects the request body and knows to parse it as a List[BikeSharingRequest] object. FastAPI knows how to parse the JSON due to the endpoint function signature:\nFilename: app/main.py\n\n@app.post('/predict', \n          summary = \"Predict bike rentals\", \n          description = \"Takes input data and returns amount rental predictions (regression).\")\n1async def predict_rentals(requests: List[BikeSharingRequest]) -&gt; dict:\n\n\n1\n\nIn the function signature, the List[BikeSharingRequest] annotation informs FastAPI to interpret the raw JSON input as a list of JSON objects that match the BikeSharingRequest schema.\n\n\n\n\n\nProcess Breakdown\n\n1. JSON Parsing and Validation\n\nFastAPI loops through the JSON objects in the request body.\nEach JSON object is validated against the BikeSharingRequest Pydantic model.\nValidation includes:\n\nEnsuring all required fields are present.\nVerifying field types and constraints.\nApplying custom validators defined in the BikeSharingRequest model.\n\n\n\n\n2. Error Handling\n\nIf any JSON object fails validation, FastAPI stops the process and returns a 422 Unprocessable Entity response, including details about the validation errors.\nThe predict_rentals function is not called if validation fails.\n\n\n\n3. Object Creation\n\nIf all JSON objects pass validation, FastAPI creates a list of BikeSharingRequest instances and passes them to the predict_rentals function as the requests argument.\n\n\n\n4. Data Shaping and Inference\n\nInside the predict_rentals function:\n\nThe input data is shaped for inference by the CatBoost model.\nFrom List(BikeSharingRequest) to numpy array.\nPredictions are generated using the trained model.\n\n\n\n\n5. Response\n\nThe predictions are formatted as a JSON response and sent back to the user.\n\n\n\n\nVisual Representation\nThe entire process is summarized in the flowchart below:\n\n\n\nVisual representation of our FastAPI app.\n\n\nFor simplicity, the Uvicorn server is abstracted away from the diagram. The focus is solely on the core FastAPI application flow, from receiving a request to sending a response.\nWith the core functionality of our FastAPI application in place, (i.e., the /predict endpoint for serving predictions), it’s time to focus on deployment and automation. While building a robust application is essential, ensuring that it can be reliably tested, built, and deployed is equally important. This is where Continuous Integration (CI) and Continuous Deployment (CD) come into play.\n\nBy implementing CI/CD pipelines, we can:\n\nAutomate Testing: ensure every change to the codebase is verified by running unit tests and integration tests automatically.\nStreamline Deployment: build and push a Docker image of the application to a container registry (e.g., Docker Hub), ready for deployment in any environment.\nMaintain Code Quality: catch bugs early and enforce consistent standards with every change.\n\nIn the next section, we’ll set up a CI/CD workflow using GitHub Actions to:\n\nRun automated tests with pytest.\nBuild and publish a Docker image of our FastAPI app.\n\nLet’s dive into automating the lifecycle of our application!"
  },
  {
    "objectID": "bikesharing_api.html#automated-tests",
    "href": "bikesharing_api.html#automated-tests",
    "title": "Bike Rentals Prediction API",
    "section": "Automated tests",
    "text": "Automated tests\nAlthough we touched on testing briefly in the CI/CD section, it’s time to dive deeper into this critical aspect of software development.\nAutomated testing is a cornerstone of modern software engineering practices. It accelerates the development process while ensuring the quality and reliability of the codebase. For example, imagine you need to refactor a clunky series of steps into a cleaner, more modular function. During this process, there’s always a risk of unintentionally introducing hard-to-detect bugs. This is where automated tests prove invaluable—they act as a safety net, quickly identifying errors and verifying that your changes didn’t break any existing functionality.\nIn this section, we’ll explore how automated tests are implemented in our project, focusing on key scenarios such as:\n\nValidating the functionality of the /predict endpoint.\nEnsuring data transformation processes work as expected.\nVerifying the behavior of the trained CatBoost model.\n\nLet’s dive into writing and running automated tests to ensure and enhance the quality of our FastAPI application! All test scripts are organized in the tests/ directory, categorized into three logical groups:\n\nAPI tests: validate the endpoints of our FastAPI app.\nInference tests: ensure the ML model behaves as expected during predictions.\nUnit tests: test isolated components, such as helper functions or data transformations.\n\n\nAPI tests\nWhile we won’t cover every example, feel free to explore the full test suite in the repository. For now, let’s focus on a test for the API.\nBelow is an example test for the /predict endpoint, which validates predictions for a single input row:\nFilename: tests/test_api.py\n\n# --snip--\n\n# Test the /predict endpoint with a single row\ndef test_predict_rentals_single():\n1    payload = {\n        \"season\": \"Summer\",\n        \"mnth\": \"June\",\n        \"hr\": 14,\n        \"holiday\": \"No\",\n        \"weekday\": \"Monday\",\n        \"workingday\": \"Yes\",\n        \"weathersit\": 1,\n        \"temp\": 0.78,\n        \"atemp\": 0.697,\n        \"hum\": 0.43,\n        \"windspeed\": 0.2537\n    }\n    response = client.post(\"/predict\", json=[payload])  \n2    assert response.status_code == 200\n    assert \"predictions\" in response.json()\n    assert isinstance(response.json()[\"predictions\"], list)\n\n# --snip--\n\n\n1\n\nSimulates a Request sends a single payload to the /predict endpoint, mimicking a real-world API request.\n\n2\n\nValidates the Response\n\n\n\n\n\nEnsures the API returns a 200 OK status, confirming successful processing.\nVerifies that the response contains a predictions key.\nChecks that the predictions are returned as a list, as expected.\n\nThis test is important for ensuring the /predict endpoint handles individual requests correctly. By automating it, we safeguard against regressions whenever the API logic or underlying model is updated.\nLet’s now move forward to explore how the inference logic is validated.\n\n\nInference tests\nInference tests are designed to verify that the predictive model is functioning correctly in terms of processing inputs and generating outputs. While these tests don’t evaluate whether the model’s predictions are accurate (a non-trivial task we will leave for the company data scientists9), they ensure that the model receives the expected input features and produces predictions in the correct format.\nThe following is an example inference test:\nFile: tests/test_inference.py\n\n# --snip--\n\n# Test model prediction with a single row\ndef test_model_single_prediction():\n    features = np.array([[\"Summer\", \"June\", 14, \"No\", \"Monday\", \"Yes\",1, 0.78, 0.697, 0.43, 0.2537]]) \n    prediction = ml_model.predict(features)\n    assert prediction is not None             \n    assert isinstance(prediction, np.ndarray) \n\n# --snip--\n\n\nInput Simulation\n\nCreates a NumPy array representing a single row of input features for the predictive model.\nMimics real-world input data to ensure the test is practical and relevant.\n\nOutput Validation\n\nConfirms that the model returns a valid prediction (i.e., not None or empty).\nVerifies that the output is a NumPy array, as expected from the CatBoost model.\n\n\nInference tests are essential for detecting issues such as:\n\nInput Format Errors: ensuring the model can process the provided features without errors.\nOutput Integrity: verifying that predictions are returned in the correct data structure (e.g., NumPy array).\nModel Loading: checking that the model is correctly loaded and operational.\n\n\n\nRunning the tests\nRunning these tests with pytest is a breeze. You just need to go to the root of the project (in a terminal) and run pytest folder_that_contain_the_tests/. So, let’s run the tests:\n\npytest tests/\n\nIf everything goes well, we will see an output like this in our terminal:\n\nuser@user-pc:~/ml_model_api$ pytest tests/\n======================================== test session starts =========================================\nplatform linux -- Python 3.12.3, pytest-8.3.4, pluggy-1.5.0\nrootdir: ~/ml_model_api\nplugins: anyio-4.7.0, cov-6.0.0\ncollected 12 items                                                                                   \n\ntests/test_api.py .....                                                                        [ 41%]\ntests/test_inference.py ...                                                                    [ 66%]\ntests/test_unit.py ....                                                                        [100%]\n\n========================================= 12 passed in 0.58s =========================================\n\nAll three set of tests were successful.\nBy running these tests, we gain confidence that the model pipeline is intact and ready for deployment. While further evaluation of the model’s predictions may require additional analysis by data scientists, our automated tests ensure that the system behaves as expected and is free from structural issues. With this foundation, we can now move on to deploying and running the microservice."
  },
  {
    "objectID": "bikesharing_api.html#containarization",
    "href": "bikesharing_api.html#containarization",
    "title": "Bike Rentals Prediction API",
    "section": "Containarization",
    "text": "Containarization"
  },
  {
    "objectID": "bikesharing_api.html#inference",
    "href": "bikesharing_api.html#inference",
    "title": "Bike Rentals Prediction API",
    "section": "Inference",
    "text": "Inference\nInference is the process of using the trained machine learning model to generate predictions based on input data. With the /predict endpoint, our FastAPI application makes it easy to perform real-time predictions for bike rentals.\n\nExample: Making a Prediction\nTo test the inference process, you can send a POST request to the /predict endpoint with the required input features. Here’s an example using curl:\n\ncurl -X POST \"http://127.0.0.1:8000/predict\" \\\n-H \"Content-Type: application/json\" \\\n-d '[\n    {\n        \"season\": \"Summer\",\n        \"mnth\": \"June\",\n        \"hr\": 14,\n        \"holiday\": \"No\",\n        \"weekday\": \"Monday\",\n        \"workingday\": \"Yes\",\n        \"weathersit\": 1,\n        \"temp\": 0.78,\n        \"atemp\": 0.697,\n        \"hum\": 0.43,\n        \"windspeed\": 0.2537\n    }\n]'\n\nThe server will respond with a prediction:\n\n{\n  \"predictions\": [225.755]\n}\n\nThis inference process demonstrates the power of combining machine learning with FastAPI. By containerizing and exposing the model through an API, we’ve created a flexible, efficient way to generate predictions in real-time. Whether for testing, integrating into other systems, or deploying to production, this pipeline ensures the model’s predictions are accessible and easy to use."
  },
  {
    "objectID": "bikesharing_api.html#microservice-up-and-running",
    "href": "bikesharing_api.html#microservice-up-and-running",
    "title": "Bike Rentals Prediction API",
    "section": "Microservice up and running",
    "text": "Microservice up and running\nOnce the application passes all tests, it’s time to deploy the microservice and get it running. To simplify this process, we’ve containerized the application using Docker, making it easy to share, run, and scale across different environments.\nFor this project, you can use the pre-built Docker image hosted on Docker Hub to quickly get the application up and running on your machine. The following steps will guide you through the deployment process:\n\nPull the Pre-Built Docker Image: download the image directly from Docker Hub.\nRun the Docker Container: start the containerized application with a single command.\nAccess the API: use the provided endpoints to interact with the microservice.\n\nLet’s dive into each step!\n\nStep 1: Pull the Image from Docker Hub\nRun the following command to download the image:\ndocker pull jospablo777/ml-model-api:latest\n\n\nStep 2: Run the Docker Container\nStart the container with the following command:\ndocker run -d -p 8000:80 --name bike-sharing-api jospablo777/ml-model-api:latest\n\n\nStep 3: Access the API\nThe API will be accessible at:\n\nBase URL: http://127.0.0.1:8000\nSwagger UI: http://127.0.0.1:8000/docs\nReDoc: http://127.0.0.1:8000/redoc\n\nYou can now send requests to the API, such as POST requests to /predict for bike rental predictions.\nWith the microservice deployed and running, you can now interact with the API to make predictions. The /predict endpoint allows you to send input data and receive predictions about bike rentals in real time. This brings us to the heart of the system: inference10."
  },
  {
    "objectID": "bikesharing_api.html#what-we-learned",
    "href": "bikesharing_api.html#what-we-learned",
    "title": "Bike Rentals Prediction API",
    "section": "What we learned?",
    "text": "What we learned?\nWow! That was a lot of information and code to dive into, wasn’t it?\nThroughout this project, we explored the end-to-end process of developing a machine learning microservice. Hopefully, you’ve picked up a few valuable skills and techniques to enhance your workflow as a data professional. Here’s a quick recap of what we covered:\n\nBuilding an API with FastAPI:\n\nWe created endpoints, validated input data with Pydantic models, and exposed our machine learning model through a /predict endpoint.\n\nContainerizing Applications:\n\nUsing Docker, we learned how to build a portable, scalable container for our app, complete with a pre-built image on Docker Hub.\n\nAutomated Testing:\n\nWe implemented unit, inference, and API tests to ensure our app is robust and reliable, catching issues before deployment.\n\nCI/CD Pipelines:\n\nBy leveraging GitHub Actions, we automated the testing, building, and deployment process, making our development cycle more efficient.\n\nReal-Time Predictions:\n\nThrough inference, we demonstrated how to use the trained CatBoost model to generate predictions in real-time via a FastAPI endpoint.\n\n\nThis journey showcased how modern tools and best practices in software engineering can streamline machine learning workflows, from development to deployment.\n\nClosing Thoughts\nWhether you’re building ML-powered APIs, automating pipelines, or just exploring FastAPI and Docker, these skills are a great foundation for developing scalable, production-ready solutions. If you have questions, ideas, or feedback, feel free to reach out—I’d love to hear from you!\nAlso, if you’re in Costa Rica, let’s connect over coffee, tea, or a beer and have a data-filled chat! Feel free to reach out to me on:\n\nLinkedIn: José Pablo Barrantes\nBlueSky: doggofan77.bsky.social\n\nThank you for following along, and happy coding! 🚀"
  },
  {
    "objectID": "bikesharing_api.html#whats-left",
    "href": "bikesharing_api.html#whats-left",
    "title": "Bike Rentals Prediction API",
    "section": "Whats left?",
    "text": "Whats left?\nIn this project, we only scratched the surface of some of the many software engineering and MLOps practices that can elevate our workflows, improve delivery processes, and increase the value we generate as data professionals. While we covered foundational concepts like API development, containerization, and CI/CD, there’s so much more to explore when it comes to bridging the gap between machine learning and production-ready systems.\nI hope that in future tutorials and practical projects we can deep dive more in MLOps subjects like:\n\nModel Monitoring: continuously tracking model performance in production to detect data drift, monitor accuracy, and ensure reliability over time.\nAutomated Retraining in CI/CD: incorporating automatic model retraining pipelines into the CI/CD process to seamlessly update models based on new data.\nModel Versioning: managing multiple versions of models to track changes, enable rollbacks when necessary, and ensure reproducibility in machine learning workflows.\nFeature Stores: centralizing feature engineering pipelines to create reusable, consistent features for training and inference.\n\nThese practices are at the heart of MLOps and represent the next steps toward creating robust, scalable, and maintainable machine learning systems. Stay tuned for more in-depth tutorials and practical projects that will dive into these topics!"
  },
  {
    "objectID": "bikesharing_api.html#recommended-readings",
    "href": "bikesharing_api.html#recommended-readings",
    "title": "Bike Rentals Prediction API",
    "section": "Recommended readings",
    "text": "Recommended readings\n\nMicroservice APIs by Jose Haro Peralta\nIf you’re new to APIs and microservices, Microservice APIs by Jose Haro Peralta provides an excellent starting point. The author presents fundamental software architecture and engineering principles in an accessible, hands-on way. You’ll find practical code examples to experiment with, paired with clear theoretical explanations that help you understand both the “how” and the “why” behind the code.\n\n\nDocker Deep Dive by Nigel Poulton\nFor anyone looking to learn Docker, Nigel Poulton’s Docker Deep Dive offers a concise yet insightful introduction. Despite its brevity, the book covers many concepts and provides straightforward exercises. By following along with the examples, you’ll quickly gain the confidence to use Docker effectively in real-world scenarios.\n\n\nSoftware Engineering for Data Scientists by Andrew Treadway\nCurrently available as a MEAP (Manning Early Access Program), Software Engineering for Data Scientists by Andrew Treadway is a remarkable resource for data professionals eager to strengthen their software engineering skills. The book’s in-progress chapters seamlessly blend theory and practice, offering well-researched explanations and practical examples. This is a must-read if you want to elevate your data projects with solid engineering principles."
  },
  {
    "objectID": "bikesharing_api.html#about-the-pondering-guy-in-the-header",
    "href": "bikesharing_api.html#about-the-pondering-guy-in-the-header",
    "title": "Bike Rentals Prediction API",
    "section": "About the pondering guy in the header",
    "text": "About the pondering guy in the header\nIt’s the wizard from “Pondering My Orb.” The art is the work by Angus Mcbride and was featured in a Lord of the Rings game book :) you can check Know Your Meme for more info/memes."
  },
  {
    "objectID": "bikesharing_api.html#footnotes",
    "href": "bikesharing_api.html#footnotes",
    "title": "Bike Rentals Prediction API",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis must be how George R. R. Martin feels when foreshadowing a major (traumatic) event in his novels.↩︎\nAt the time of writing, I am aware that “no” is the answer since we’re still just at the introduction of this tutorial.↩︎\nIt’s even worse today, in our modern remote era. The dreaded Microsoft Teams ringtone will constantly haunt you.↩︎\nYou probably already noticed that we keep bringing up the link to this notebook. That’s because we spent so much time on it, and we were hoping you could look at our precious.↩︎\nI learned this tip for technical writing from “The Rust Programming Language” book by Steve Klabnik, Carol Nichols, and the Rust Community. The book is a masterpiece and a highly recommended reading; you can find it here.↩︎\nWhoops, here is the notebook again. When presenting results to my stakeholders as a habit, I always prepare two decks, one with very high-level results and a second highly technical one with all the math of the statistical/ML models and intricacies of the system design, just in case somebody has a question on the interesting technical part. This is expecting a deep, nerdy discussion about models, gaps, opportunities, and science; to date, the counts for the times I’ve used this second deck remain at 0. The good news is that it might be a proxy of our stakeholders’ trust in us.↩︎\nエンチラーダをいただけますか？↩︎\nA decorator is a function whose intention is to modify the behavior of other functions; it takes the original function and converts it into a different function. Luciano Ramalho does a great job explaining decorators in chapter 9 of his book “Fluent Python”.↩︎\nProbably you, as there aren’t many due to budget reasons.↩︎\nLove this word. The definition that I like the most is “driving conclusions given some evidence (data).”↩︎"
  },
  {
    "objectID": "bikesharing_api.html#cicd",
    "href": "bikesharing_api.html#cicd",
    "title": "Bike Rentals Prediction API",
    "section": "CI/CD",
    "text": "CI/CD\nContinuous Integration (CI) and Continuous Deployment (CD)"
  },
  {
    "objectID": "bikesharing_api.html#citation",
    "href": "bikesharing_api.html#citation",
    "title": "Bike Rentals Prediction API",
    "section": "Citation",
    "text": "Citation\nNo citation is required, but if you found this project helpful, I’d greatly appreciate a mention! 😄"
  },
  {
    "objectID": "bikesharing_api.html#cicd-continuous-integration-continuos-deployment",
    "href": "bikesharing_api.html#cicd-continuous-integration-continuos-deployment",
    "title": "Bike Rentals Prediction API",
    "section": "CI/CD (Continuous Integration / Continuos Deployment)",
    "text": "CI/CD (Continuous Integration / Continuos Deployment)\nContinuous Integration (CI) and Continuous Deployment (CD) are essential pillars of modern software development, enabling teams to automate testing, building, and deployment processes. They ensure that every change to the codebase is tested and deployed efficiently.\nWhile I’d love to provide an in-depth introduction to CI/CD here, I highly recommend the excellent tutorial by Itskmyoo, titled Automating FastAPI Project Build with GitHub Actions and Push to DockerHub. It offers a clear, practical guide to setting up CI/CD pipelines for FastAPI projects using GitHub Actions.\n\nOur Customization\nBuilding on Itskmyoo’s tutorial, we’ll implement a similar CI/CD pipeline with minor adjustments tailored to our project. Below are the configurations you’ll need to get started.\n\nDockerfile (Containerization)\nThe Dockerfile is critical for containerizing our FastAPI app. Here’s the updated version for our project:\nFilename: Dockerfile\n\n# Use an official Python runtime as a parent image\nFROM python:3.12-slim\n\n# Set the working directory in the container\nWORKDIR /api_app\n\n# Copy the requirements file into the container\nCOPY requirements.txt /api_app/\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the project files into the container\nCOPY . /api_app/\n\n# Expose port 80 for external access\nEXPOSE 80\n\n# Define environment variable\nENV NAME=ml-model-api-docker\n\n# Set the maintainer label\nLABEL maintainer=\"jospablo777 &lt;jospablo777@gmail.com&gt;\"\n\n# Run the FastAPI app with Uvicorn\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n\n\n\nGitHub Actions Workflow\nThe GitHub Actions workflow (.github/workflows/main.yml) automates testing, building, and pushing the Docker image to Docker Hub. Below is the configuration for our project:\nFilename: .github/workflows/main.yml\n\nname: ML Model API - Test, build, and push\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: \"3.12\"\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run tests\n      run: pytest tests/ --cov=app\n\n  build:\n    needs: test # build job runs only if tests pass\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Set up Docker Buildx\n      uses: docker/setup-buildx-action@v1\n\n    - name: Login to Docker Hub\n      run: echo ${{ secrets.DOCKERHUB_ACCESS_TOKEN }} | docker login -u ${{ secrets.DOCKERHUB_USERNAME }} --password-stdin\n\n    - name: Build and push Docker image\n      run: |\n        docker buildx create --use\n        docker buildx build \\\n          --file Dockerfile \\\n          --tag jospablo777/ml-model-api:latest \\\n          --push .\n\n    env:\n      DOCKER_CLI_EXPERIMENTAL: enabled\n      DOCKER_BUILDKIT: 1\n\nThe main modification was adding a testing section before the docker build.\n\nHow It Works\n\nTesting:\n\nRuns on every push to the main branch.\nInstalls dependencies and runs pytest to verify the codebase, ensuring that only tested and reliable code proceeds to the build stage.\n\nBuilding:\n\nAfter passing the tests, the build job creates a Docker image using the specified Dockerfile.\nThe image is tagged and pushed to Docker Hub.\n\nEnvironment Variables:\n\nThe pipeline uses DOCKERHUB_ACCESS_TOKEN and DOCKERHUB_USERNAME, securely stored in GitHub Secrets, to authenticate with Docker Hub.\n\n\n\n\n\nFinal Steps\nWith the guidance from Itskmyoo’s tutorial and the customizations above, our CI/CD pipeline is ready to go. Every push to the main branch will:\n\nRun automated tests.\nBuild a Docker image.\nPush the image to Docker Hub for seamless deployment.\n\nThis process is illustrated here:\n\n\n\nCI/CD configuration for our project.\n\n\nThis setup ensures your application is always deployable and meets quality standards."
  },
  {
    "objectID": "bikesharing_api.html#acknowledgements",
    "href": "bikesharing_api.html#acknowledgements",
    "title": "Bike Rentals Prediction API",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work was possible thanks to Little Mai, my home office manager. She is always full of support and fresh ideas, a best in class professional—setting the gold standard in both dedication and cuteness. From offering moral support to keeping me on schedule (and occasionally demanding breaks), Little Mai has been an indispensable part of this journey.\n\n\n\nLittle Mai"
  }
]